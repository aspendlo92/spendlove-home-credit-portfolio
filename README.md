# spendlove-home-credit-portfolio
This portfolio summarizes my contributions and key takeaways from my capstone project in the MSBA program at the University of Utah, where we developed predictive models to address the business challenge presented in the [Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk/data) project on Kaggle. 

# Table of Contents

1. [Business Problem and Objective](#business-problem-and-objective)
2. [Solution to the Business Problem](#solution-to-the-business-problem)
3. [My Contribution](#my-contribution)
4. [Business Value of the Solution](#business-value-of-the-solution)
5. [Difficulties Encountered](#difficulties-encountered)
6. [What I Learned](#what-i-learned)

## Business Problem and Objective

Home Credit, a financial services provider, is focused on enhancing financial inclusion by offering loans to individuals who have limited or no formal credit history. However, the lack of traditional credit data makes it difficult to assess the repayment capacity of these borrowers accurately. To address this challenge, Home Credit seeks to create a more reliable method of predicting credit default, by way of predictive modeling. By developing a more reliable model, Home Credit can strike the balance between approving loans for these creditworthy, but typically underserved applicants while still avoiding loans for those at high risk of default. This will in turn improve client satisfaction and representation while also supporting sustainable lending practices by minimizing loan defaults.

## Solution to the Business Problem

To meet this business objective, our group set out to develop a predictive model for Home Credit to accurately classify these loan applicants, leveraging a variety of client data beyond traditional credit scores. After a thorough process of data exploration, cleaning, and initial model evaluation, we selected three modeling techniques well suited to working with our highly dimensional and imbalanced data (elastic net logistic regression, support vector machines, and random forests) with which we'd try to create the most accurate model. In the end, after many iterations and testing, one of our random forest models performed best internally, achieving a 96% accuracy and AUC of 0.98, while our SVM model performed the best when applied to Kaggle's test data, achieving an AUC of 0.673.

## My Contribution

My primary contribution to this project was creating and authoring the notebook that we submitted for the Modeling assignment (see: spendlove-home-credit-portfolio/notebooks/spendlove-modeling.Rmd). I took all the code for data cleaning, exploration, and model creation—which was previously scattered across multiple files in our GitHub repository—and synthesized it into a single, well-polished, well-flowing and readable notebook. I created the structure of the notebook, wrote all of the explanatory text and in-chunk comments (which totalled about 800 lines), and ensured it all flowed logically. My goal was to transform the coding, primarily owned and authored by Adam and Georgia, into an intelligible document that not only detailed *what* we did but also explained *why* we did it and the significance of our findings, such that anyone could download the notebook, run it, and understanding what we understood about the data and our models.

To fully understand each step of the process so I could synthesize this single notebook, I actively reviewed, ran, and provided feedback on Georgia and Adam’s R models as they were writing them and wrote additional code to bridge gaps between the sections, ensuring the notebook was cohesive. I also made sure the notebook met all rubric criteria, particularly in the introduction and conclusion, where I outlined our goals, findings, and potential improvements for future iterations.

We then took my summarizing writing and the structure of this notebook and, together, we created the PowerPoint deck for our in-person presentation of our models and conclusions. This portion of the project felt purely collaborative, where each one of us was contributing in equal measure to the ideas (as we met in real time via Zoom) on the design of the deck, the visual aesthetics that would make it catchy and legible, and the writing and fleshing-out of each slide. We were particularly proud of the bold, contrasting red-black aesthetic that we landed on together, after several iterations, and our use of images and symbols in place of blocks of text. 

## Business Value of the Solution

Our current recommendation for Home Credit is the support vector machine (SVM) model, which again showed a promising AUC of 0.673 on real-world, imbalanced test data. While this model’s performance is not as high as we'd like, it still offers valuable insights and significantly improves Home Credit’s ability to assess loan default risk in the short term. It balances precision and recall well, making it a useful tool for assessing borrowers who may otherwise be overlooked or misclassified. Our work also provides valuable insight into how the model can be improved in concrete, actionable ways--namely, by correctly balancing the data by only applying oversampling to the training split and using the provided supplemental data sources to find additional, strong predictors. These recommended next steps will likely lead to a more reliable and generalizable solution that better serves Home Credit's mission of extending credit to underbanked populations while minimizing defaults. 

## Difficulties Encountered

As mentioned, a major difficulty we encountered was the discovery of how poorly our random forest model performed on the test data when we obtained our Kaggle score. This model had performed so well in the training phase that we didn't test it on Kaggle until late in the timeline of creating and submitting the notebook, confident that all of our work to balance the data and rigorously test and tune the models would translate to a high AUC once applied to the test data. All of the signs throughout the training process led us to believe our model didn't have an overfitting issue. So, when we saw the significant drop in AUC from 0.98 on the training data to 0.60 on the test, we had to work long hours the rest of the day to try to diagnose the issue, try running the test data through our other models, summarize this additional work, and determine those concrete, actionable next steps that I write about in the conclusion. Additionally, I experienced some personal difficulty at the beginning of the project that led to me joining the work later than expected. The main result of this seemed to be that we had less bandwidth to try to incorporate the data from the supplemental sources on Kaggle, which could've improved the model further before we submitted it.

## What I Learned

This project provided invaluable insights to me both in my technical understanding of the process of creating a predictive model and in my professional experience collaborating on models like this. On the more technical side of things, I learned how different a model's performance may be on real-world data in comparison to the training data, highlighting how crucial validation is. It's now obvious to me that it was rather naive to assume that our Kaggle score would turn out high, even given good signs and results prior to validation, and that I should've suggested completing that part of the process sooner, as part of our iterating on different models. I also gained a lot of experience with writing and creating these models in R, as I collaborated with my team members on their final models that I'd then need to explain in my writing. 

On the professional side of things, I gained extremely valuable experience working in a group on highly technical, complex models like this. It took a lot of communication and time-management to make sure we were all on the same page about the direction we were following in the project each day and in keeping track of the dozens of updates, GitHub commits, and file versions that all melded into our final product. The whole process of collaborating in real-time on the PowerPoint was really rewarding, where it felt like we were chiseling out our final product from the marble of blank, white slides, as was practicing our in-person presentation, which we practiced over and over until we were a well-oiled machine.
